#!/bin/bash -l
#SBATCH --job-name=orca-job ### name of your job
#SBATCH --output=%x.o%j
#SBATCH --error=%x.e%j
#SBATCH --nodes=1 ### only one node on woody 
#SBATCH --ntasks-per-node=4 
##SBATCH --cpus-per-task=10 
#SBATCH --time=23:59:00
#SBATCH --export=NONE

############################################################
# change input name below here
############################################################

 jobname="neb_Rh3phenP_180" #name of your input file without ending
 workdir="."

############################################################
# change nothing below here unless you are sure
############################################################

unset SLURM_EXPORT_ENV

module purge
#module load intel/2021.4.0

module load mkl/2021.4.0
module load orca/6.0.1-avx2
module list

export ppn=${SLURM_NTASKS_PER_NODE}
echo `scontrol show hostnames ${SLURM_JOB_NODELIST}`
echo ${SLURM_JOB_PARTITION}
echo "Number of OpenMP threads:     " $SLURM_CPUS_PER_TASK
echo "Number of MPI procs per node: " $SLURM_NTASKS_PER_NODE
echo "Number of MPI processes:      " $SLURM_NTASKS

# enter the SLURM work directory:
cd $SLURM_SUBMIT_DIR

# set path for ORCA executable
export ORCA=`which orca`
export ORCA2MKL=`which orca_2mkl`

echo $ORCA
echo $ORCA2MKL

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export MKL_NUM_THREADS=1
export OMPI_MCA_hwloc_base_report_bindings=true

# copy to local workdir:
if [ "${workdir}" != "." ]; then
  mkdir -p "${workdir}"
  cp "${jobname}.inp" "${workdir}"
  cd "${workdir}"
fi

# wrapper for mpirun
cat << EOF >> ${workdir}/mpirun
srun \$(echo "\$*" | sed 's/^\-np/\-n/')
EOF

chmod +x ${workdir}/mpirun
export PATH=${workdir}:${PATH}

echo -n "Starting run at: "
date

$ORCA ${jobname}.inp > $SLURM_SUBMIT_DIR/${jobname}.out

$ORCA2MKL ${jobname} - molden

echo -n "Stopping run at: "
date

# move results and tidy up:
echo "move remaining temporary files to SLURM_SUBMIT_DIR"
rm -f mpirun

if [ ${workdir} != "." ]; then
  cp -a * $SLURM_SUBMIT_DIR
  cd $SLURM_SUBMIT_DIR
  rm -rf ${workdir}
fi



